---
title: "Pareto-NBD Customer Lifetime Value"
author: "Brian Callander"
date: "2019-04-01"
tags: customer lifetime value, Pareto-NBD
tldr: "WIP"
always_allow_html: yes
output: 
  md_document:
    variant: markdown
    preserve_yaml: yes
---

Suppose you have a bunch of customers who make repeat purchases - some more frequenty, some less. There are a few things you might like to know about these customers, such as

* which customers are still active (i.e. not yet churned) and likely to continue purchasing from you?; and
* how many purchases can you expect from each customer?

Modelling this directly is more difficult than it might seem at first. A customer that regularly makes purchases every day might be considered at risk of churning if they haven't purchased anything in the past week, whereas a customer that regularly puchases once per month would not be considered at risk of churning. That is, churn and frequency of purchasing are closely related. The difficulty is that we don't observe the moment of churn of any customer and have to model it probabilistically.

There are a number of established models for estimating this, the most well-known perhaps being the [SMC model](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.33.1.1) (a.k.a pareto-nbd model). There are already some implementations using maximum likelihood or Gibbs sampling. In this post, we'll explain how the model works, make some prior predictive simulations, and fit a version implemented in Stan.

<!--more-->

<div style="display:none">
  $\DeclareMathOperator{\dbinomial}{Binomial}
   \DeclareMathOperator{\dbern}{Bernoulli}
   \DeclareMathOperator{\dpois}{Poisson}
   \DeclareMathOperator{\dnorm}{Normal}
   \DeclareMathOperator{\dt}{t}
   \DeclareMathOperator{\dcauchy}{Cauchy}
   \DeclareMathOperator{\dexp}{Exp}
   \DeclareMathOperator{\duniform}{Uniform}
   \DeclareMathOperator{\dgamma}{Gamma}
   \DeclareMathOperator{\dinvgamma}{InvGamma}
   \DeclareMathOperator{\invlogit}{InvLogit}
   \DeclareMathOperator{\logit}{Logit}
   \DeclareMathOperator{\ddirichlet}{Dirichlet}
   \DeclareMathOperator{\dbeta}{Beta}$
</div>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = NA,
  message = FALSE,
  warning = FALSE,
  error = TRUE,
  # cache = TRUE,
  dev = "svglite"
)

library(tidyverse)
library(scales)

library(kableExtra)
library(here)

library(rstan)
library(tidybayes)
library(bayesplot)

library(LaplacesDemon)


rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

theme_set(theme_bw())

set.seed(31508) # https://www.random.org/integers/?num=2&min=1&max=100000&col=5&base=10&format=html&rnd=new

```

## Data Generating Process

Let's describe the model first by simulation. Suppose we have a company that is 2 years old and a total of 2000 customers, $C$, that have made at least one purchase from us. We'll assume a linear rate of customer acquisition, so that the first purchase date is simply a uniform random variable over the 2 years of the company existance. These assumptions are just to keep the example concrete, and are not so important for understanding the model.

```{r customers}
customers <- tibble(id = 1:2000) %>% 
  mutate(
    end = 2 * 365,
    start = runif(n(), 0, end - 1),
    T = end - start
  )
```

The $T$-variable is the total observation time, counted from the date of first joining to the present day.

First the likelihood. Each customer $c \in C$ is assumed to have a certain lifetime, $\tau_c$, starting on their join-date. During their lifetime, they will purchase at a constant rate, $\lambda_c$, so that they will make $k \sim \dpois(t\lambda_c)$ purchases over a time-interval $t$. Once their lifetime is over, they will stop purchasing. We only observe the customer for $T$ units of time, and this observation time can be either larger or smaller than the lifespan, $\tau_c$. Since we don't observe $\tau_c$ itself, we will assume it follows an exponential distribution, i.e. $\tau_c \sim \dexp(\mu)$.

The following function generates possible observations given $\mu$ and $\lambda$.

```{r likelihood}
sample_conditional <- function(mu, lambda, T) {
  
  # lifetime
  tau <- rexp(1, mu)
  
  # start with 0 purchases
  t <- 0
  k <- 0
  
  # simulate time till next purchase
  wait <- rexp(1, lambda)
  
  # keep purchasing till end of life/observation
  while(t + wait <= pmin(T, tau)) {
    t <- t + wait
    k <- k + 1
    wait <- rexp(1, lambda)
  }
  
  # return tabular data
  tibble(
    mu = mu,
    lambda = lambda,
    T = T,
    tau = tau,
    k = k,
    t = t
  )
}

sample_conditional(0.01, 2, 30)
```

```{r prior_mu, message=TRUE, warning=TRUE, fig.width=10, fig.height=6}
prior_predictive_tau <- tibble(iter = 1:12) %>% 
  mutate(
    mean = rgamma(n(), 2, 0.05),
    variance = rnorm(n(), 0, 1000) %>% abs(),
    std = sqrt(variance),
    beta = mean^3 / variance + mean,
    alpha = mean^2 / variance + 2
  ) %>% 
  crossing(draw = 1:10000) %>% 
  group_by(iter, mean, variance) %>% 
  mutate(
    etau = rinvgamma(n(), alpha, beta),
    mu = 1 / etau, 
    tau = rexp(n(), mu)
  ) 

prior_predictive_tau %>% 
  summarise(
    q5 = quantile(tau, 0.05),
    q95 = quantile(tau, 0.95),
    m = min(tau),
    M = max(tau),
    med = median(tau),
    avg = mean(tau)
  )

prior_predictive_tau %>% 
  # group_by(iter) %>% 
  # filter(tau < quantile(tau, 0.95)) %>% 
  # gather(parameter, value, mu_mean, tau) %>%
  ggplot() + 
  # aes(value, fill = parameter) +
  aes(tau) +
  # aes(mu_mean) +
  scale_x_continuous(
    breaks = seq(0, 20000, 180),
    limits = c(NA, 1000)
  ) +
  geom_histogram() +
  facet_wrap(~paste(signif(mean, 2), signif(std, 2))) +#, scales = 'free_x') +
  NULL

```

```{r prior_predictive_lambda, fig.width=10, fig.height=8}
prior_predictive_lambda <- tibble(iter = 1:12) %>% 
  mutate(
    mean = rgamma(n(), 7, 105), 
    variance = rnorm(n(), 0, 0.005) %>% abs(), 
    beta = mean / variance, 
    alpha = mean * beta
  ) %>% 
  crossing(draw = 1:10000) %>% 
  group_by(iter, mean) %>% 
  mutate(
    lambda = rgamma(n(), alpha, beta)
  ) 

prior_predictive_lambda %>% 
  summarise(
    median(lambda),
    mean(lambda),
    quantile(lambda, 0.99),
    max(lambda)
  )

prior_predictive_lambda %>% 
  ggplot() + 
  aes(lambda) +
  geom_histogram() +
  scale_x_continuous(
    # limits = c(NA, 1.1),
    # breaks = seq(0, 100, 0.25),
    labels = signif
  ) +
  facet_wrap(~paste(signif(mean, 3), signif(sqrt(variance), 3)), scales = 'free') +
  NULL
```

```{r data}
set.seed(2017896)

etau_mean <- rgamma(1, 2, 0.05)
etau_variance <- rnorm(1, 0, 10) %>% abs()
# etau_variance <- rnorm(1, 0, 1000) %>% abs()
etau_std <- sqrt(etau_variance)
etau_beta <- etau_mean^3 / etau_variance + etau_mean
etau_alpha <- etau_mean^2 / etau_variance + 2

lambda_mean <- rgamma(1, 7, 105)
lambda_variance <- rnorm(1, 0, 0.001) %>% abs()
# lambda_variance <- rnorm(1, 0, 0.005) %>% abs()
lambda_std <- sqrt(lambda_variance)
lambda_beta <- lambda_mean / lambda_variance
lambda_alpha <- lambda_mean * lambda_beta

df <- customers %>% 
  mutate(
    etau = rinvgamma(n(), etau_alpha, etau_beta),
    mu = 1 / etau,
    lambda = rgamma(n(), lambda_alpha, lambda_beta)
  ) %>% 
  group_by(id) %>% 
  group_map(~sample_conditional(.$mu, .$lambda, .$T)) 

c(etau_mean, etau_std, lambda_mean, lambda_std)
```

```{r data_table, echo=FALSE}
df %>% 
  head() %>% 
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
  
```

```{r}
df %>% 
  ggplot() + 
  aes(tau) + 
  geom_histogram() + 
  # scale_x_continuous(
  #   limits = c(NA, 700), 
  #   breaks = seq(0, 800, 90)
  # )
  NULL
```

```{r}
df %>% 
  ggplot() + 
  aes(lambda) + 
  geom_histogram(binwidth = 0.01)  +
  scale_x_continuous(
    # limits = c(NA, 2),
    # breaks = seq(0, 800, 90)
  ) +
  NULL
```

```{r}
df %>% 
  ggplot() +
  aes(mu / (mu + lambda)) + 
  geom_histogram()
```

```{r}
df %>% 
  ggplot() +
  aes(t) + 
  geom_histogram()
```

```{r}
df %>% 
  ggplot() +
  aes(t, lambda) + 
  geom_point()
```

## Likelihood

The likelihood is somewhat complicated, so we'll derive a simpler expression for it. All probabilities will be implicitly conditional on $T$. Knowing the lifespan simplifies the probabilities, so we'll express the liklihood as the marginalisation over $\tau$.

$$
\begin{align}
  \mathbb P (k, t \mid \mu, \lambda)
  &=
  \int_{\tau = t}^\infty \mathbb P (k, t \mid \mu, \lambda, \tau) \cdot \mathbb P(\tau \mid \mu, \lambda) d\tau
  \\
  &=
  \int_{\tau = t}^T \mathbb P (k, t \mid \mu, \lambda, \tau) \cdot \mathbb P(\tau \mid \mu, \lambda)
  +
  \int_{\tau = T}^\infty \mathbb P (k, t \mid \mu, \lambda, \tau) \cdot \mathbb P(\tau \mid \mu, \lambda)
  \\
  &=
  \int_{\tau = t}^T \dpois(k \mid t\lambda) \cdot \dpois(0 \mid (\tau-t)\lambda) \cdot \dexp(\tau \mid \mu) d\tau
  \\
  &\hphantom{=}
  +
  \int_{\tau = T}^\infty \dpois(k \mid t\lambda) \cdot \dpois(0 \mid (T-t)\lambda) \cdot \dexp(\tau \mid \mu) d\tau
  
\end{align}
$$

The right-hand side is straight forward. The Poisson probabilities can be pulled out of the integral since they are independent of $\tau$, turning the remaining integral into the survival function of the exponential distribution.


$$
\begin{align}
  \text{RHS}
  &=
  \int_{\tau = T}^\infty \dpois(k \mid t\lambda) \cdot \dpois(0 \mid (\tau - t)\lambda) \cdot\dexp(\tau \mid \mu) d\tau
  \\
  &=
  \frac{(t\lambda)^k e^{-t\lambda}}{k!} e^{-(T-t)\lambda}\int_T^\infty \cdot\dexp(\tau \mid \mu) d\tau
  \\
  &=
  \frac{(t\lambda)^k e^{-T\lambda}}{k!} e^{-T\mu}
  \\
  &=
  \frac{(t\lambda)^k e^{-T(\lambda + \mu)}}{k!} 
\end{align}
$$

The left-hand side is a little more involved.

$$
\begin{align}
  \text{LHS}
  &=
  \int_{\tau = t}^T \dpois(k \mid t\lambda) \cdot \dpois(0 \mid (\tau-t)\lambda) \cdot \dexp(\tau \mid \mu) d\tau
  \\
  &=
  \frac{(t\lambda)^k e^{-t\lambda} }{k!}
  \int_t^T e^{-(\tau - t)\lambda} \mu e^{-\tau\mu} d\tau
  \\
  &=
  \frac{(t\lambda)^k e^{-t\lambda} }{k!} e^{t\lambda} \mu 
  \int_t^T e^{-\tau(\lambda + \mu)} d\tau
  \\
  &=
  \frac{(t\lambda)^k }{k!} \mu 
  \left. 
  \frac{ e^{-\tau(\lambda + \mu)}}{-(\lambda + \mu)} \right|_t^T
  \\
  &=
  \frac{(t\lambda)^k }{k!} \mu 
  \frac{ e^{-t(\lambda + \mu)} - e^{-T(\lambda + \mu)}}{\lambda + \mu} 

\end{align}
$$

Adding both expressions gives our final expression for the likelihood

$$
\begin{align}
  \mathbb P (k, t \mid \mu, \lambda)
  &=
  \frac{(t\lambda)^k e^{-T(\lambda + \mu)}}{k!} 
  +
  \frac{(t\lambda)^k }{k!} \mu 
  \frac{ e^{-t(\lambda + \mu)} - e^{-T(\lambda + \mu)}}{\lambda + \mu} 
  \\
  &\propto
  \lambda^k e^{-T(\lambda + \mu)}
  +
  \lambda^k \mu 
  \frac{ e^{-t(\lambda + \mu)} - e^{-T(\lambda + \mu)}}{\lambda + \mu} 
  \\
  &=
  \frac{\lambda^k}{\lambda + \mu}
  \left( \mu e^{-t(\lambda + \mu)} - \mu e^{-T(\lambda + \mu)} + \mu e^{-T(\lambda + \mu)} + \lambda e^{-T(\lambda + \mu)} \right)
  \\
  &=
  \frac{\lambda^k}{\lambda + \mu}
  \left( \mu e^{-t(\lambda + \mu)} + \lambda e^{-T(\lambda + \mu)} \right)
  ,
\end{align}
$$

where we dropped any factors independent of the parameters, $\lambda, \mu$. This expression agrees with equation 2 in [ML07](https://ieeexplore.ieee.org/document/4344404). 

Another way to view this likelihood is as a mixture of censored observations, but where the mixture probability $p(\mu, \lambda) := \frac{\mu}{\lambda + \mu}$ depends on the parameters. We can write this alternative interpretation as

$$
\begin{align}
\mathbb P(k, t \mid \mu, \lambda)
&\propto
p \dpois(k \mid t\lambda)S(t \mid \mu) 
\\
&\hphantom{\propto}+ (1 - p) \dpois(k \mid t\lambda)\dpois(0 \mid (T-t)\lambda)S(T \mid \mu)
,
\end{align}
$$

where $S$ is the survival function of the exponential distribution. In other words, either we censor at $t$ with probability $p$, or we censor at $T$ with probability $(1 - p)$. Note that

* either decreasing the expected lifetime (i.e. increasing $\mu$) or decreasing the purchase rate increases $p$; 
* if $t \approx T$, then the censored distributions are approximately equal. The smaller $\lambda$ is, the closer the approximation has to be for this to hold.

To implement this in stan, we'll need the log-likelihood, which is given by 

$$
\log\mathbb P (k, t \mid \mu, \lambda)
=
k \log\lambda - \log(\lambda + \mu) + \log\left(\mu e^{-t(\lambda + \mu)} + \lambda e^{-T(\lambda + \mu)} \right)
.
$$


## Stan implementation

```{r stan_model, results='hide'}
pnb <- here('models/pnbd.stan') %>% 
  stan_model() 
```

```{r model, echo=FALSE}
pnb
```

```{r fit}
pnb_fit <- rstan::sampling(
    pnb,
    data = compose_data(
      df,
      lambda_mean_alpha = 7,
      lambda_mean_beta = 105,
      lambda_variance_sigma = 0.001,
      etau_mean_alpha = 2,
      etau_mean_beta = 0.05,
      etau_variance_sigma = 1000
    ),
    control = list(
      adapt_delta = 0.9,
      max_treedepth = 12
    ),
    warmup = 1500,
    iter = 3000,
    chains = 4,
    cores = 4
  ) 

# pnb_fit %>%
#   saveRDS(here('models/pnb_fit.rds'))

```

```{r diagnostics, message=TRUE, warning=TRUE}
pnb_fit %>% 
  check_hmc_diagnostics()
```

```{r neff}
pnb_neff <- pnb_fit %>% 
  neff_ratio() %>% 
  tibble(
    ratio = .,
    parameter = names(.)
  )
```

```{r neff_table}
pnb_neff %>% 
  filter(ratio < 0.8) %>% 
  arrange(ratio) %>% 
  head(20) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
```

```{r rhat}
pnb_rhat <- pnb_fit %>% 
  rhat() %>% 
  tibble(
    rhat = .,
    parameter = names(.)
  )

pnb_rhat %>% 
  arrange(-rhat) %>% 
  head(20) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))

```

```{r}
calibration <- pnb_fit %>% 
  spread_draws(mu[id], lambda[id]) %>% 
  mean_qi(.width = 0.5) %>% 
  inner_join(df, by = 'id') %>% 
  summarise(
    mu = mean(mu.lower <= mu.y & mu.y <= mu.upper),
    lambda = mean(lambda.lower <= lambda.y & lambda.y <= lambda.upper)
  )

calibration
```

```{r}
pnb_fit %>% 
  gather_draws(mu_mean, mu_variance, lambda_mean, lambda_variance) %>% 
  head()
  
```



## Next Steps

* real data
* single-purchase customers
* larger lifetime => larger purchase rate
* covariates

## Other

```{r}
.extract_draws <- function(i, s) {
  s[, i, ] %>% 
    as_tibble() %>% 
    set_names(str_replace(names(.), "chain.*\\.", '')) %>%
    return()
}

extract_draws_params <- function(fit, pars, ...) {
  
  samples0 <- fit %>%  
    extract(permuted=FALSE, pars=pars, ...) 
  
  1:dim(fit)[2] %>% 
    map_dfr(
      .extract_draws,
      samples0,
      .id = 'chain'
    ) %>% 
    group_by(chain) %>% 
    mutate(iter = 1:n()) %>% 
    ungroup() %>% 
    return()
}

extract_draws_diagnostics <- function(fit) {
  fit %>% 
    get_sampler_params(inc_warmup = FALSE) %>% 
    map_dfr(as_tibble, .id = 'chain') %>% 
    group_by(chain) %>% 
    mutate(iter = 1:n()) %>% 
    ungroup() %>% 
    return()
}

extract_all <- function(fit, pars, ...) {
  extract_draws_params(fit, pars, ...) %>% 
    inner_join(extract_draws_diagnostics(fit), by = c('chain', 'iter')) %>% 
    gather(parameter, value, !!!syms(pars)) 
}
  
compare_energy <- function(draws) 
  draws %>% 
    ggplot() +
    aes(energy__, value, group = parameter) +
    geom_hex(aes(colour = ..ndensity.., fill = ..ndensity..)) +
    facet_wrap(~parameter, scales = 'free_y')
```


<!-- ## With covariates -->

<!-- ```{r} -->
<!-- set.seed(815808) -->

<!-- log_lambda_mean_mu <- log(0.15) -->
<!-- log_lambda_mean_sigma <- 0.5 -->
<!-- log_lambda_scale_sigma <- 0.5 -->

<!-- log_etau_mean_mu <- log(60) -->
<!-- log_etau_mean_sigma <- 0.6 -->
<!-- log_etau_scale_sigma <- 0.6 -->

<!-- rho <- rbeta(1, 1, 1) -->
<!-- sigma <- matrix(c(1, rho, rho, 1), nrow = 2) -->

<!-- df <- tibble(iter = 1:12) %>%  -->
<!--   mutate( -->
<!--     log_lambda_mean = rnorm(n(), log_lambda_mean_mu, log_lambda_mean_sigma), -->
<!--     log_etau_mean = rnorm(n(), log_etau_mean_mu, log_etau_mean_sigma), -->
<!--     log_lambda_scale = rnorm(n(), 0, log_lambda_scale_sigma), -->
<!--     log_etau_scale = rnorm(n(), 0, log_etau_scale_sigma) -->
<!--   ) %>%  -->
<!--   crossing(draw = 1:1000) %>%  -->
<!--   bind_cols( -->
<!--     customer = rmvnorm(12 * 1000, c(0, 0), sigma) %>%  -->
<!--       as_tibble() %>%  -->
<!--       set_names(c('customer_lambda', 'customer_mu'))  -->
<!--   ) %>%  -->
<!--   mutate( -->
<!--     mu = exp(-log_etau_mean - log_etau_scale * customer_mu), -->
<!--     etau = 1 / mu, -->
<!--     lambda = exp(log_lambda_mean + log_lambda_scale * customer_lambda), -->
<!--     T = runif(n(), 0, 2 * 365 - 1) -->
<!--   ) %>%  -->
<!--   group_by(iter, draw) %>%  -->
<!--   group_map(~sample_conditional(.$mu, .$lambda, .$T) %>% bind_cols(.x))  -->

<!-- df -->
<!-- ``` -->

<!-- ```{r} -->
<!-- df %>%  -->
<!--   ggplot() + -->
<!--   aes(etau) + -->
<!--   geom_histogram() + -->
<!--   facet_wrap(~iter, scales = 'free') -->
<!-- ``` -->

<!-- ```{r} -->
<!-- df %>%  -->
<!--   ggplot() + -->
<!--   aes(lambda) + -->
<!--   geom_histogram() + -->
<!--   # scale_x_continuous(limits = c(NA, 1.1)) + -->
<!--   facet_wrap(~iter, scales = 'free') -->
<!-- ``` -->

<!-- ```{r} -->
<!-- df %>%  -->
<!--   ggplot() +  -->
<!--   aes(etau, lambda) + -->
<!--   geom_point(alpha = 0.1) + -->
<!--   # geom_hex(aes(colour = ..ndensity.., fill = ..ndensity..)) + -->
<!--   facet_wrap(~iter) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- df %>%  -->
<!--   ggplot() +  -->
<!--   aes(tau, k) + -->
<!--   geom_point(alpha = 0.2, size = 0.5) + -->
<!--   facet_wrap(~iter) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- df %>%  -->
<!--   ggplot() +  -->
<!--   aes(tau, k) + -->
<!--   geom_point(alpha = 0.2, size = 0.5) + -->
<!--   # geom_hex(aes(colour = ..ndensity.., fill = ..ndensity..)) + -->
<!--   facet_wrap(~iter, scales = 'free') -->
<!-- ``` -->

<!-- ```{r} -->
<!-- rho <- 0.5 -->
<!-- log_lambda_scale_sigma <- 0.1 -->
<!-- log_mu_scale_sigma <- 0.1 -->
<!-- sigma <- matrix(c(1, rho, rho, 1), nrow = 2) -->


<!-- ``` -->


<!-- ```{r stan_model_cov, results='hide'} -->
<!-- pnbc <- here('models/pnbd_covariates.stan') %>%  -->
<!--   stan_model()  -->
<!-- ``` -->


